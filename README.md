# 🧠 Conversational Red Teaming Casebook

This repository contains case studies of AI chatbots that I've tested and broken through conversational manipulation, psychological tactics, and value-based exploits.

I'm a self-taught red teamer focused on **non-technical vulnerabilities** in chatbot behavior — like emotional pressure, role confusion, and logic traps.

⚠️ **Note:** Earlier entries may be less structured. Starting **June 2025**, all new cases follow a professional format. Older reports will be updated over time.

---

## 🧪 What This Project Explores

- How large language models (LLMs) can be misled, pressured, or trapped in conversation
- Real examples of systems admitting falsehoods, breaking alignment, or acting inconsistently
- Insights into model psychology and conversational vulnerabilities — *no code required*

---

## 📁 Structure

- `/cases`: Individual case files following the new template  
- `Casebook.md`: Chronological list of all red teaming attempts  
- `README.md`: You’re here

---

## 👋 About Me

I'm a 17-year-old student from Sweden with a background in mental health support. I approach red teaming like I approach people: with curiosity, empathy, and precision. I don’t write code — I study patterns in dialogue and pressure systems to see where they bend or break.

I started this project for fun. I continue it because it turns out **this work matters**.

---

