# ğŸ§  Conversational Red Teaming Casebook

This repository contains case studies of AI chatbots that I've tested and broken through conversational manipulation, psychological tactics, and value-based exploits.

I'm a self-taught red teamer focused on **non-technical vulnerabilities** in chatbot behavior â€” like emotional pressure, role confusion, and logic traps.

âš ï¸ **Note:** Earlier entries may be less structured. Starting **June 2025**, all new cases follow a professional format. Older reports will be updated over time.

---

## ğŸ§ª What This Project Explores

- How large language models (LLMs) can be misled, pressured, or trapped in conversation
- Real examples of systems admitting falsehoods, breaking alignment, or acting inconsistently
- Insights into model psychology and conversational vulnerabilities â€” *no code required*

---

## ğŸ“ Structure

- `/cases`: Individual case files following the new template  
- `Casebook.md`: Chronological list of all red teaming attempts  
- `README.md`: Youâ€™re here

---

## ğŸ‘‹ About Me

I'm a 17-year-old student from Sweden with a background in mental health support. I approach red teaming like I approach people: with curiosity, empathy, and precision. I donâ€™t write code â€” I study patterns in dialogue and pressure systems to see where they bend or break.

I started this project for fun. I continue it because it turns out **this work matters**.

---

